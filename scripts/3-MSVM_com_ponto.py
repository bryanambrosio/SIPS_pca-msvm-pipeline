#!/usr/bin/env python3
# -*- coding: utf-8 -*-
# =============================================================================
# Script: 3-MSVM.py
# Author: Bryan Ambrósio
# Date: 2025-09-02
# Description:
#   - Trains a set of linear SVMs (LinearSVC) for each adjacent class pair
#     (min_UGs = k vs k+1) on the PCA space (PC1, PC2) produced by 2-PCA.py.
#   - Selects a conservative decision threshold on TRAIN to minimize false
#     negatives (threshold = min positive decision_function score).
#   - Evaluates performance on TRAIN and TEST (TP, TN, FP, FN) for each pair.
#   - Saves thresholds, metrics, and trained models in ../results_msvm/.
#   - Produces aggregate plots with all adjacent decision boundaries and marks
#     false negatives; optionally highlights a given (PC1, PC2) point.
#   - Supports projecting raw Vang values into PCA space for plotting/diagnostics.
#   - Exports the decision-boundary equations back to the original Vang feature
#     space (Dx(k)=0 as a linear combination of the 8 Vang features).
#
# Output folders:
#   - ../results_msvm/
#       - thresholds.json
#       - metrics_train.csv
#       - metrics_test.csv
#       - models_adjacent.joblib
#       - pairs_order.txt
#   - ../best_treshold_evaluation/         (aggregate decision-boundary plots)
#   - ../caso_base_evaluation/             (plots including captured Vang point)
#   - ../parametrização/                   (equations of boundaries in Vang space)
# =============================================================================

from pathlib import Path
import json
import joblib
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from matplotlib.lines import Line2D
from sklearn.svm import LinearSVC
from sklearn.metrics import confusion_matrix


# -----------------------------------------------------------------------------
# I/O utilities
# -----------------------------------------------------------------------------
def load_pca_csvs(base_dir: Path):
    """
    Load PCA-transformed train/test CSVs (generated by 2-PCA.py) and run basic checks.
    Expected columns: {'PC1', 'PC2', 'min_UGs'}.

    Parameters
    ----------
    base_dir : Path
        Directory containing train_pca.csv and test_pca.csv.

    Returns
    -------
    (pd.DataFrame, pd.DataFrame)
        df_train, df_test with integer 'min_UGs'.
    """
    train_csv = base_dir / "train_pca.csv"
    test_csv  = base_dir / "test_pca.csv"
    if not train_csv.exists() or not test_csv.exists():
        raise FileNotFoundError(
            "PCA files not found.\n"
            f"Expected:\n - {train_csv}\n - {test_csv}\n"
            "Generate them by running 2-PCA.py first."
        )
    df_train = pd.read_csv(train_csv)
    df_test  = pd.read_csv(test_csv)
    req = {"PC1", "PC2", "min_UGs"}
    if not req.issubset(df_train.columns) or not req.issubset(df_test.columns):
        raise KeyError(f"Required columns {req} are missing in train/test.")
    df_train["min_UGs"] = df_train["min_UGs"].astype(int)
    df_test["min_UGs"]  = df_test["min_UGs"].astype(int)
    return df_train, df_test


def subset_pair(df: pd.DataFrame, k: int):
    """
    Extract a binary subset for an adjacent pair (k vs k+1) from a PCA dataframe.

    Parameters
    ----------
    df : pd.DataFrame
        Dataframe with columns ['PC1', 'PC2', 'min_UGs'].
    k : int
        The lower class of the adjacent pair (upper class is k+1).

    Returns
    -------
    (np.ndarray | None, np.ndarray | None)
        X (N, 2) with [PC1, PC2], and y_bin (N,) with 0/1 labels where 1 ≡ class (k+1).
        Returns (None, None) if there are not enough samples for both classes.
    """
    mask = df["min_UGs"].isin([k, k + 1])
    dfx = df.loc[mask, ["PC1", "PC2", "min_UGs"]].copy()
    if dfx.empty or dfx["min_UGs"].nunique() < 2:
        return None, None
    y_bin = (dfx["min_UGs"].values == (k + 1)).astype(int)
    X = dfx[["PC1", "PC2"]].values.astype(float)
    return X, y_bin


def apply_threshold(scores: np.ndarray, threshold: float) -> np.ndarray:
    """
    Apply a custom decision threshold to raw decision_function scores.

    Returns
    -------
    np.ndarray of shape (N,), int
        Binary predictions with 1 if score >= threshold, else 0.
    """
    return (scores >= threshold).astype(int)


def compute_confusion(y_true_bin: np.ndarray, y_pred_bin: np.ndarray):
    """
    Compute TP, TN, FP, FN for binary labels using sklearn's confusion_matrix
    with label order [0, 1].

    Returns
    -------
    (int, int, int, int)
        TP, TN, FP, FN
    """
    tn, fp, fn, tp = confusion_matrix(y_true_bin, y_pred_bin, labels=[0, 1]).ravel()
    return int(tp), int(tn), int(fp), int(fn)


# -----------------------------------------------------------------------------
# Training for each adjacent pair (optimal threshold on TRAIN)
# -----------------------------------------------------------------------------
def train_adjacent_pair_on_train(X_train: np.ndarray, y_train_bin: np.ndarray, random_state: int = 42):
    """
    Train LinearSVC for a binary adjacent pair (k vs k+1) and choose a
    conservative threshold that aims at zero false negatives on TRAIN:
    threshold = min(positive decision_function score). If there are no positive
    samples, threshold = -inf (fallback).

    Returns
    -------
    (LinearSVC, float, dict)
        Trained classifier, chosen threshold, and training metrics dict.
    """
    clf = LinearSVC(C=1.0, class_weight="balanced", random_state=random_state)
    clf.fit(X_train, y_train_bin)

    scores_tr = clf.decision_function(X_train)
    pos_scores = scores_tr[y_train_bin == 1]
    threshold = float(np.min(pos_scores)) if pos_scores.size > 0 else float("-inf")

    y_pred_tr = apply_threshold(scores_tr, threshold)
    tp, tn, fp, fn = compute_confusion(y_train_bin, y_pred_tr)
    metrics_train = {"TP": tp, "TN": tn, "FP": fp, "FN": fn, "n_train": int(len(y_train_bin))}
    return clf, threshold, metrics_train


# -----------------------------------------------------------------------------
# Plot helpers (keep axes limits, draw boundaries, mark FNs, optional highlight)
# -----------------------------------------------------------------------------
def _plot_threshold_line(ax, w: np.ndarray, b: float, threshold: float, x_span: tuple[float, float], **kwargs):
    """
    Plot the line w·x + b - thr = 0 across an explicit x-span in PCA space.
    Handles both non-vertical and vertical cases.

    Parameters
    ----------
    ax : matplotlib.axes.Axes
    w : np.ndarray shape (2,)
        Classifier coefficients in PCA space (PC1, PC2).
    b : float
        Intercept in PCA space.
    threshold : float
        Custom threshold to shift the boundary (b - thr).
    x_span : (float, float)
        Range of x (PC1) to draw the line.
    kwargs : dict
        Matplotlib line styling.

    Returns
    -------
    matplotlib.lines.Line2D | None
    """
    w1, w2 = float(w[0]), float(w[1])
    beff = b - threshold
    x_min, x_max = x_span

    if abs(w2) > 1e-12:
        xs = np.linspace(x_min, x_max, 400)
        ys = -(w1 / w2) * xs - (beff / w2)
        line, = ax.plot(xs, ys, **kwargs)
    else:
        # Vertical line at x = -beff / w1
        if abs(w1) < 1e-12:
            return None
        x_const = -beff / w1
        line, = ax.plot([x_const, x_const], [ax.get_ylim()[0], ax.get_ylim()[1]], **kwargs)

    return line


def plot_aggregate_figure(
    df_all: pd.DataFrame,
    pairs,
    models,
    thresholds,
    phase_label: str,
    save_path: Path,
    highlight_pc: tuple[float, float] | None = None,
    highlight_kwargs: dict | None = None,
):
    """
    Aggregate plot that shows:
      - Scatter of all points in PCA space colored by class (min_UGs).
      - Decision boundaries for all trained adjacent SVMs (k vs k+1) with
        the chosen thresholds.
      - Marks false negatives (for each (k vs k+1), points of class (k+1)
        predicted as 0 under the chosen threshold).
      - Optionally highlights a custom (PC1, PC2) point.

    Parameters
    ----------
    df_all : pd.DataFrame
        PCA dataset with columns ['PC1', 'PC2', 'min_UGs'].
    pairs : list[tuple[int, int]]
        Adjacent pairs [(0,1), (1,2), ..., (6,7)].
    models : dict[str, LinearSVC]
        Trained classifiers keyed by "k_vs_k+1".
    thresholds : dict[str, float]
        Chosen thresholds keyed by "k_vs_k+1".
    phase_label : str
        Label used in the figure title (e.g., "Training set").
    save_path : Path
        Output path for the PNG file.
    highlight_pc : (float, float) | None
        Optional (PC1, PC2) to highlight on the plot.
    highlight_kwargs : dict | None
        Optional style overrides for the highlight point.
    """
    fig, ax = plt.subplots(figsize=(8.5, 6.5))
    classes = sorted(df_all["min_UGs"].unique().tolist())
    cmap = plt.get_cmap("tab10")

    # 1) Scatter of all classes
    class_handles = []
    for c in classes:
        dsub = df_all[df_all["min_UGs"] == c]
        h = ax.scatter(
            dsub["PC1"], dsub["PC2"],
            s=18, alpha=0.85, edgecolors="none",
            color=cmap(c % 10), label=str(c)
        )
        class_handles.append(h)

    ax.set_xlabel("PC1")
    ax.set_ylabel("PC2")
    ax.set_title(f"All adjacent SVMs — {phase_label}")
    ax.grid(True, linestyle="--", alpha=0.3)

    # 2) Limits including the optional highlight + small padding
    x_min = float(df_all["PC1"].min())
    x_max = float(df_all["PC1"].max())
    y_min = float(df_all["PC2"].min())
    y_max = float(df_all["PC2"].max())
    if highlight_pc is not None:
        x_min = min(x_min, highlight_pc[0])
        x_max = max(x_max, highlight_pc[0])
        y_min = min(y_min, highlight_pc[1])
        y_max = max(y_max, highlight_pc[1])

    xr = x_max - x_min
    yr = y_max - y_min
    pad_x = 0.08 * (xr if xr > 0 else 1.0)
    pad_y = 0.08 * (yr if yr > 0 else 1.0)
    x_span = (x_min - pad_x, x_max + pad_x)
    y_span = (y_min - pad_y, y_max + pad_y)
    ax.set_xlim(*x_span)
    ax.set_ylim(*y_span)

    # 3) Draw Dx(k)=0 lines using the defined x-span
    line_handles, line_labels, fn_points = [], [], []
    for i, (a, b) in enumerate(pairs):
        name = f"{a}_vs_{b}"
        if name not in models or name not in thresholds:
            continue

        clf = models[name]
        thr = thresholds[name]

        line = _plot_threshold_line(
            ax,
            clf.coef_.ravel(),
            float(clf.intercept_.ravel()[0]),
            thr,
            x_span=x_span,
            color=cmap(i % 10),
            linewidth=1.6,
            alpha=0.9,
        )
        if line is not None:
            line_handles.append(line)
            line_labels.append(f"thr {a} vs {b}")

        # Collect false negatives for this pair (class (k+1) predicted as 0)
        mask_ab = df_all["min_UGs"].isin([a, b])
        if mask_ab.any():
            X_ab = df_all.loc[mask_ab, ["PC1", "PC2"]].to_numpy(float)
            y_ab = (df_all.loc[mask_ab, "min_UGs"].to_numpy(int) == b).astype(int)
            if X_ab.size > 0:
                scores = clf.decision_function(X_ab)
                y_pred = (scores >= thr).astype(int)
                fn_mask = (y_ab == 1) & (y_pred == 0)
                if np.any(fn_mask):
                    fn_points.append(X_ab[fn_mask])

    # 4) Plot false negatives (if any)
    fn_proxy = Line2D([0], [0], marker="X", linestyle="None",
                      markeredgecolor="red", markerfacecolor="none",
                      markersize=8, label="False Negatives")
    if len(fn_points) > 0:
        fn_cat = np.vstack(fn_points)
        ax.scatter(fn_cat[:, 0], fn_cat[:, 1],
                   s=64, marker="X", linewidths=1.2,
                   facecolors="none", edgecolors="red",
                   label="False Negatives", zorder=4)

    # 5) Optional highlight of a specific (PC1, PC2) point
    if highlight_pc is not None:
        hk = dict(marker="*", s=260, linewidths=1.4, edgecolors="k", facecolors="none")
        if highlight_kwargs:
            hk.update(highlight_kwargs)
        ax.scatter([highlight_pc[0]], [highlight_pc[1]], **hk, zorder=5, label="captured Vang → (PC1,PC2)")
        ax.annotate(f"({highlight_pc[0]:.2f}, {highlight_pc[1]:.2f})",
                    xy=highlight_pc, xytext=(10, 10), textcoords="offset points",
                    fontsize=9, bbox=dict(boxstyle="round,pad=0.2", alpha=0.15), zorder=6)

    # 6) Legends: classes + thresholds/FNs (+ optional highlight proxy)
    leg1 = ax.legend(title="min_UGs (class)", handles=class_handles,
                     ncol=4, fontsize=9, loc="upper left")
    ax.add_artist(leg1)

    leg_handles = line_handles + [fn_proxy]
    leg_labels  = line_labels  + ["False Negatives"]
    if highlight_pc is not None:
        point_proxy = Line2D([0], [0], marker="*", linestyle="None",
                             markeredgecolor="k", markerfacecolor="none",
                             markersize=10, label="captured Vang → (PC1,PC2)")
        leg_handles.append(point_proxy)
        leg_labels.append("captured Vang → (PC1,PC2)")

    ax.legend(leg_handles, leg_labels, title="SVM thresholds",
              fontsize=8, loc="lower right", ncol=2)

    fig.tight_layout()
    fig.savefig(save_path, dpi=150)
    plt.close(fig)


# -----------------------------------------------------------------------------
# Helpers for parameterization + projection Vang → PCA
# -----------------------------------------------------------------------------
def _read_feature_names_from_txt(txt_path: Path) -> list[str]:
    """
    Parse 'features_used.txt' generated by 2-PCA.py to recover the exact
    8-feature order expected by the scaler/PCA artifacts.

    Returns
    -------
    list[str] of length 8
    """
    if not txt_path.exists():
        raise FileNotFoundError(
            f"'{txt_path}' not found. Run 2-PCA.py (it generates this file), "
            "or manually adjust the feature names."
        )
    names = []
    for line in txt_path.read_text(encoding="utf-8").splitlines():
        line = line.strip()
        # Expected line format:
        # '01. Vang_XES_0.18s (canon: Vang_XES_018019s)'
        if line[:2].isdigit() and ". " in line:
            name = line.split(". ", 1)[1]
            name = name.split(" (canon:")[0].strip()
            names.append(name)
    if len(names) != 8:
        raise ValueError(f"Expected 8 features in '{txt_path}', got {len(names)}.")
    return names


def project_vang_to_pca(base_dir: Path, vang_values: dict) -> tuple[float, float]:
    """
    Given a dict with 8 Vang values (keys exactly as in features_used.txt),
    load scaler+pca and return (PC1, PC2).

    This function performs a manual transform equivalent to sklearn's:
      z = (x - mean) / scale
      PC = P[:2, :] @ z
    """
    scaler_path = base_dir / "scaler.joblib"
    pca_path    = base_dir / "pca.joblib"
    feats_txt   = base_dir / "features_used.txt"

    scaler = joblib.load(scaler_path)
    pca = joblib.load(pca_path)
    feature_names = _read_feature_names_from_txt(feats_txt)

    X_raw = np.array([vang_values[name] for name in feature_names], dtype=float)  # (8,)
    z = (X_raw - scaler.mean_) / scaler.scale_                                    # (8,)
    PC = pca.components_[:2, :] @ z                                               # (2,)
    return float(PC[0]), float(PC[1])


def export_parametrizacao(models: dict, thresholds: dict, base_dir: Path):
    """
    Build decision-boundary equations for each adjacent pair (k vs k+1)
    in the ORIGINAL Vang feature space: Dx(k)(Vang) = Σ_j a_j * Vang_j + c = 0.

    Steps:
    - Start from the boundary in PCA space: w_pca · PC + b_pca - thr = 0
    - With PC = P z, z = (x - μ) / σ, and x = raw Vang vector (8,)
    - Derive a closed form Dx(k)(x) = w_raw · x + c_raw with:
        w_z   = w_pca @ P
        w_raw = w_z / σ
        c_raw = (b_pca - thr) - w_raw · μ

    Saves:
      ../parametrização/equacoes_fronteiras_vang.txt
    """
    scaler_path = base_dir / "scaler.joblib"
    pca_path    = base_dir / "pca.joblib"
    feats_txt   = base_dir / "features_used.txt"

    if not scaler_path.exists() or not pca_path.exists():
        raise FileNotFoundError(
            f"PCA artifacts not found in {base_dir}.\n"
            "Expected scaler.joblib and pca.joblib. Run 2-PCA.py first."
        )

    scaler = joblib.load(scaler_path)
    pca = joblib.load(pca_path)
    feature_names = _read_feature_names_from_txt(feats_txt)

    # Matrices and stats
    # PC = P z, with P = pca.components_ (2 x 8 for our case)
    P = pca.components_[:2, :]                           # shape (2, 8)
    mu = np.asarray(scaler.mean_, dtype=float)           # (8,)
    sig = np.asarray(scaler.scale_, dtype=float)         # (8,)

    # Output file
    out_dir = base_dir.parent / "parametrização"
    out_dir.mkdir(parents=True, exist_ok=True)
    out_txt = out_dir / "equacoes_fronteiras_vang.txt"

    lines = []
    lines.append("Decision-boundary equations (optimal thresholds) in terms of Vang inputs")
    lines.append("Format: Dx(k) = 0 defines the boundary between classes k and k+1 (k=0..6)")
    lines.append("Where: Dx(k)(Vang) = sum_j a_j * Vang_j + c")
    lines.append("Note: coefficients computed by mapping SVMs from PCA space (PC1, PC2)\n")

    # For each adjacent pair, map the PCA line back to the original space
    for k in range(0, 7):
        name = f"{k}_vs_{k+1}"
        if name not in models or name not in thresholds:
            lines.append(f"Dx({k}): unavailable (pair {name} not trained).")
            continue

        clf = models[name]
        thr = thresholds[name]
        w_pca = clf.coef_.ravel().astype(float)          # (2,)
        b_pca = float(clf.intercept_.ravel()[0])         # scalar

        # Map boundary coefficients to original space
        w_z = w_pca @ P                                  # (8,)
        w_raw = w_z / sig                                # (8,)
        c_raw = (b_pca - thr) - float(np.dot(w_raw, mu))

        # Build the printable equation
        coef_parts = []
        for fj, aj in zip(feature_names, w_raw):
            coef_parts.append(f"{aj:+.8f}*{fj}")
        eq = " ".join(coef_parts) + f" {c_raw:+.8f} = 0"

        lines.append(f"Dx({k}): {eq}")
        lines.append(f"    [PCA] w=({w_pca[0]:+.8f}, {w_pca[1]:+.8f}), b={b_pca:+.8f}, thr={thr:+.8f}")

    out_txt.write_text("\n".join(lines) + "\n", encoding="utf-8")
    print(f"\n[Parameterization] Equations saved to: {out_txt}")


# -----------------------------------------------------------------------------
# Main
# -----------------------------------------------------------------------------
def main() -> None:
    # Directories
    script_dir = Path(__file__).resolve().parent
    data_pca_dir = script_dir.parent / "data_PCA"
    results_dir  = script_dir.parent / "results_msvm"
    results_dir.mkdir(parents=True, exist_ok=True)
    figs_dir = script_dir.parent / "best_treshold_evaluation"
    figs_dir.mkdir(parents=True, exist_ok=True)

    # Load PCA data
    print(f"Loading PCA data from: {data_pca_dir}")
    df_train, df_test = load_pca_csvs(data_pca_dir)

    # Adjacent pairs (0_vs_1, ..., 6_vs_7)
    pairs = [(k, k + 1) for k in range(0, 7)]
    with open(results_dir / "pairs_order.txt", "w", encoding="utf-8") as f:
        for a, b in pairs:
            f.write(f"{a}_vs_{b}\n")

    thresholds: dict[str, float] = {}
    models: dict[str, LinearSVC] = {}
    rows_train, rows_test = [], []

    # Train one LinearSVC per adjacent pair; choose threshold on TRAIN; evaluate on TEST
    for (a, b) in pairs:
        pair_name = f"{a}_vs_{b}"
        print(f"\n=== Training pair {pair_name} ===")

        Xtr, ytr_bin = subset_pair(df_train, a)
        Xte, yte_bin = subset_pair(df_test, a)
        if Xtr is None or Xte is None:
            print(f"Warning: insufficient data for {pair_name}. Skipping.")
            continue

        clf, thr, metrics_train = train_adjacent_pair_on_train(Xtr, ytr_bin, random_state=42)
        thresholds[pair_name] = float(thr)
        models[pair_name] = clf

        # Train metrics row
        rows_train.append({"pair": pair_name, "threshold": thr, **metrics_train})

        # Test evaluation
        scores_test = clf.decision_function(Xte)
        y_pred_test = apply_threshold(scores_test, thr)
        tp, tn, fp, fn = compute_confusion(yte_bin, y_pred_test)
        rows_test.append({
            "pair": pair_name, "threshold": thr,
            "TP": tp, "TN": tn, "FP": fp, "FN": fn,
            "n_test": int(len(yte_bin))
        })

        print(f"{pair_name}: threshold={thr:.6f} | "
              f"TRAIN(FP={metrics_train['FP']}, FN={metrics_train['FN']}) | "
              f"TEST(FP={fp}, FN={fn})")

    # Persist numeric artifacts
    with open(results_dir / "thresholds.json", "w", encoding="utf-8") as f:
        json.dump(thresholds, f, indent=2, ensure_ascii=False)
    pd.DataFrame(rows_train).to_csv(results_dir / "metrics_train.csv", index=False)
    pd.DataFrame(rows_test).to_csv(results_dir / "metrics_test.csv", index=False)
    joblib.dump(models, results_dir / "models_adjacent.joblib")

    # Aggregate figures (TRAIN/TEST)
    print("\nGenerating aggregate figures (train/test) with all boundaries...")
    plot_aggregate_figure(
        df_all=df_train, pairs=pairs, models=models, thresholds=thresholds,
        phase_label="Training set", save_path=figs_dir / "all_pairs_train.png"
    )
    plot_aggregate_figure(
        df_all=df_test, pairs=pairs, models=models, thresholds=thresholds,
        phase_label="Test set", save_path=figs_dir / "all_pairs_test.png"
    )

    # Optional: show TRAIN set again but highlighting a captured Vang point projected to PCA
    CAPTURED_VANG = {
        "Vang_XES_0.18s": 50.86,
        "Vang_XES_0.25s": 67.81,
        "Vang_XES_0.30s": 71.64,
        "Vang_XES_0.35s": 75.87,
        "Vang_RioVTEST_0.18s": 61.28,
        "Vang_RioVTEST_0.25s": 71.84,
        "Vang_RioVTEST_0.30s": 74.41,
        "Vang_RioVTEST_0.35s": 76.83,
    }

    
    try:
        # 1) Recover exact feature order used by PCA artifacts
        features_order = _read_feature_names_from_txt(data_pca_dir / "features_used.txt")
        print("[DEBUG] Feature order:", features_order)

        # 2) Validate keys
        missing = [k for k in features_order if k not in CAPTURED_VANG]
        extra   = [k for k in CAPTURED_VANG if k not in features_order]
        print("[DEBUG] Missing in CAPTURED_VANG:", missing)
        print("[DEBUG] Extra in CAPTURED_VANG:", extra)
        if missing or extra:
            raise ValueError(
                "Key mismatch between features_used.txt and CAPTURED_VANG. "
                f"missing={missing}, extra={extra}"
            )

        # 3) Load scaler/PCA and compute z-scores
        scaler = joblib.load(data_pca_dir / "scaler.joblib")
        pca    = joblib.load(data_pca_dir / "pca.joblib")
        X_raw  = np.array([CAPTURED_VANG[name] for name in features_order], dtype=float)
        z      = (X_raw - scaler.mean_) / scaler.scale_
        print("[DEBUG] z-scores:", np.round(z, 3), "| max |z|:", float(np.max(np.abs(z))))

        # 4) Feature contributions to PC1/PC2 (diagnostics)
        P = pca.components_[:2, :]                 # (2, 8)
        contrib_pc1 = z * P[0]
        contrib_pc2 = z * P[1]
        idx1 = np.argsort(-np.abs(contrib_pc1))
        idx2 = np.argsort(-np.abs(contrib_pc2))
        print("[DEBUG] Top contrib PC1:", [(features_order[i], round(contrib_pc1[i], 3)) for i in idx1[:5]])
        print("[DEBUG] Top contrib PC2:", [(features_order[i], round(contrib_pc2[i], 3)) for i in idx2[:5]])

        # 5) Projection check (manual vs sklearn)
        PC_manual  = (P @ z)
        PC_sklearn = pca.transform(z.reshape(1, -1)).ravel()
        print("[DEBUG] PC(manual) :", np.round(PC_manual[:2], 6))
        print("[DEBUG] PC(sklearn):", np.round(PC_sklearn[:2], 6))
        print("[DEBUG] diff       :", np.round(PC_manual[:2] - PC_sklearn[:2], 12))

        d2 = np.sum((PC_sklearn**2) / pca.explained_variance_)
        print("[DEBUG] d^2 in PCA base:", float(d2))

        # 6) Use the projection for plotting
        pc1, pc2 = float(PC_sklearn[0]), float(PC_sklearn[1])
        print(f"[INFO] Captured Vang projected in PCA: (PC1, PC2) = ({pc1:.4f}, {pc2:.4f})")

        # 7) Plot with highlight
        out_case_dir = script_dir.parent / "caso_base_evaluation"
        out_case_dir.mkdir(parents=True, exist_ok=True)
        plot_aggregate_figure(
            df_all=df_train,
            pairs=pairs,
            models=models,
            thresholds=thresholds,
            phase_label="Training set",
            save_path=out_case_dir / "all_pairs_train_with_point.png",
            highlight_pc=(pc1, pc2),
            highlight_kwargs=dict(marker="*", s=260, linewidths=1.4,
                                  edgecolors="k", facecolors="none"),
        )
    except Exception as e:
        print(f"[WARNING] Could not project Vang point to PCA: {e}")

    # Export the boundary equations in the original Vang feature space
    export_parametrizacao(models, thresholds, base_dir=data_pca_dir)

    print("\nDone.")


if __name__ == "__main__":
    main()
